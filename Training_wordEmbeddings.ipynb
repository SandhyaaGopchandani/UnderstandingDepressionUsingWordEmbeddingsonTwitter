{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import multiprocessing\n",
    "import nltk\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import gensim.downloader as api\n",
    "import gzip\n",
    "import gzip\n",
    "import requests\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweet Preprocessing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def camel_case_split(identifier):\n",
    "    # https://stackoverflow.com/a/29920015/2799941\n",
    "    matches = re.finditer('.+?(?:(?<=[a-z])(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)', identifier)\n",
    "    return [m.group(0) for m in matches]\n",
    "\n",
    "\n",
    "def process_hashtag(ht):\n",
    "    ht_body = ht.group()[1:]\n",
    "    if ht_body.upper() == ht_body:\n",
    "        return \"<HASHTAG> {ht_body} <ALLCAPS>\"\n",
    "    return \"<HASHTAG> \" + \" \".join(camel_case_split(ht_body))\n",
    "\n",
    "\n",
    "def remove_emoji(tweet):\n",
    "    \"\"\"\"\n",
    "    Utility function to remove emojis from tweet text using simple regex statements..\n",
    "    \"\"\"\n",
    "    emoji_pattern = re.compile(\"[\" u\"\\U0001f680-\\U0001f6FF\"\n",
    "                               u\"\\U0001f300-\\U0001f64F\"\n",
    "                               u\"\\U00002600-\\U000027BF\"\n",
    "                               u\"\\U00002700-\\U000027BF\"\n",
    "                               u\"\\U00002600-\\U000026FF\"\n",
    "                               u\"\\U0001f600-\\U0001f64F\"\n",
    "                               u\"\\U0001f300-\\U0001f5fF\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U0001F914\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    text = emoji_pattern.sub(r'', tweet)  # no emoji\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    \"\"\"\n",
    "    Utility function to remove punctuations from tweet text using simple regex statements..\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r'(?<=\\w)[^\\s\\w\\'\\>](?![^\\s\\w])', '', tweet)\n",
    "    # text = re.sub(r'[^\\w\\s]', ' ', tweet)\n",
    "    return text\n",
    "\n",
    "\n",
    "def preprocess(tweet_text):\n",
    "    \"\"\" Run raw tweet JSON through preprocessing/cleaning steps.\n",
    "        Adapted from Stanford Glove site's Ruby tweet processing script.\n",
    "        See: https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb\n",
    "    \"\"\"\n",
    "    eyes = r\"[8:=;]\"\n",
    "    nose = r\"['`\\-]?\"\n",
    "    # remove urls\n",
    "    url = lambda tweet: re.sub(r\"https?://\\S+\\b|www\\.(\\w+\\.)+\\S*\", \"<URL>\", tweet)\n",
    "    # remove newline characters \\n\n",
    "    newline = lambda tweet: re.sub(r\"\\n\", \"\", tweet)\n",
    "    # replace @ mentions with marker\n",
    "    user = lambda tweet: re.sub(r\"@\\w+\", \"<USER>\", tweet)\n",
    "    # replace emojis with markers\n",
    "    smile = lambda tweet: re.sub(\"{eyes}{nose}[)d]+|[)d]+{nose}{eyes}\", \"<SMILE>\", tweet, re.I)\n",
    "    lolface = lambda tweet: re.sub(r\"{eyes}{nose}p+\", \"<LOLFACE>\", tweet, re.I)\n",
    "    sadface = lambda tweet: re.sub(r\"{eyes}{nose}\\(+|\\)+{nose}{eyes}\", \"<SADFACE>\", tweet)\n",
    "    neutralface = lambda tweet: re.sub(r\"{eyes}{nose}[\\\\/|l]\", \"<NEUTRALFACE>\", tweet)\n",
    "    # force slash-combined tokens to be two tokens\n",
    "    slash = lambda tweet: re.sub(r\"/\", \" / \", tweet)\n",
    "    heart = lambda tweet: re.sub(\"[<|&lt;]3\", \"<HEART>\", tweet)\n",
    "    # replace &amp; with \"and\" plus <AMP> marker\n",
    "    amp = lambda tweet: re.sub(\"&amp;\", \"and <AMP>\", tweet)\n",
    "    # replace year with marker (not very clean...)\n",
    "    year = lambda tweet: re.sub(\"(?:17|18|19|20)\\d{2}\", \"<YEAR>\", tweet)\n",
    "    # replace time with marker\n",
    "    time = lambda tweet: re.sub(\"\\d{1,2}:\\d{2}(\\s?)(am|pm)?\", r\"<TIME>\\1\", tweet)\n",
    "    # replace numbers with marker\n",
    "    number = lambda tweet: re.sub(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<NUMBER>\", tweet)\n",
    "    # add hashtag markers and indicator of whether all-caps or not\n",
    "    # also split hashtags if joined capitalized words: BlackLivesMatter -> Black Lives Matter\n",
    "    hashtag = lambda tweet: re.sub(r\"#\\S+\", process_hashtag, tweet)\n",
    "    # replace repeated characters (!!!) with single character and <REPEAT> marker\n",
    "    repeat = lambda tweet: re.sub(\"([!?.]){2,}\", r\"\\1 <REPEAT>\", tweet)\n",
    "    # replace elongated words (wayyyy) with original word and <ELONG> marker\n",
    "    elong = lambda tweet: re.sub(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <ELONG>\", tweet)\n",
    "    # replace double spaces with single space\n",
    "    spaces = lambda tweet: re.sub(\"\\s+\", \" \", tweet)\n",
    "\n",
    "    parse_funcs = [url, newline, user, smile,\n",
    "                   lolface, sadface, neutralface, slash, heart,\n",
    "                   amp, year, time, number, hashtag, repeat,\n",
    "                   elong, spaces]\n",
    "\n",
    "    for f in parse_funcs:\n",
    "        tweet_text = f(tweet_text)\n",
    "\n",
    "    tweet_text = remove_emoji(tweet_text)\n",
    "    tweet_text = remove_punctuation(tweet_text)\n",
    "\n",
    "    return tweet_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading tweet files and appending to tweet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_tweet_files(directory_name, tweet_list):\n",
    "    print(\"before\", len(tweet_list))\n",
    "    # fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    print('dir', directory_name)\n",
    "    # path = os.path.join(fileDir + directory_name)\n",
    "    # print('path', path)\n",
    "\n",
    "    good_tweets = 0\n",
    "    bad_tweets = 0\n",
    "\n",
    "    for file in os.listdir(directory_name):\n",
    "        if not file.startswith('.'):\n",
    "            date = file[:-4]\n",
    "            print(date)\n",
    "            with open(directory_name + '/' + file, 'r') as f:\n",
    "                tweets = f.readlines()\n",
    "\n",
    "            for tweetStr in tweets:\n",
    "                try:\n",
    "                    tweet = json.loads(tweetStr.strip().lower())\n",
    "                    if 'body' in tweet:\n",
    "                        text_key = 'body'\n",
    "                    elif 'text' in tweet:\n",
    "                        text_key = 'text'\n",
    "                    tweet_text = preprocess(tweet[text_key])\n",
    "                    if 'rt' not in tweet_text:\n",
    "                        #print(tweet_text)\n",
    "                        # tweet_text_eng = \" \".join(w for w in nltk.wordpunct_tokenize(tweet_text) if w.lower() in words)\n",
    "                        tweet_list.append(tweet_text.split())\n",
    "                        good_tweets += 1\n",
    "                except:\n",
    "                    bad_tweets += 1\n",
    "\n",
    "    print(good_tweets, bad_tweets)\n",
    "    print(\"after\", len(tweet_list))\n",
    "    return tweet_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_gzip_tweet_files(directory_name, tweet_list):\n",
    "    print(\"before\", len(tweet_list))\n",
    "    # fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    print('dir', directory_name)\n",
    "    # path = os.path.join(fileDir + directory_name)\n",
    "    # print('path', path)\n",
    "\n",
    "    good_tweets = 0\n",
    "    bad_tweets = 0\n",
    "\n",
    "    for file in os.listdir(directory_name):\n",
    "        if not file.startswith('.'):\n",
    "            with gzip.open(directory_name + '/' + file, 'rb') as f:\n",
    "                tweets = f.readlines()\n",
    "                print(file, len(tweets))\n",
    "\n",
    "            for tweetStr in tweets:\n",
    "                try:\n",
    "                    tweet = json.loads(tweetStr.strip().lower())\n",
    "                    print(tweet)\n",
    "                    if 'body' in tweet:\n",
    "                        text_key = 'body'\n",
    "                    elif 'text' in tweet:\n",
    "                        text_key = 'text'\n",
    "                    tweet_text = preprocess(tweet[text_key])\n",
    "                    if 'rt' not in tweet_text:\n",
    "                        print(tweet_text)\n",
    "                        #tweet_text_ = \" \".join(w for w in nltk.wordpunct_tokenize(tweet_text) if w.alpha())\n",
    "                        # tweet_text_eng = \" \".join(w for w in nltk.wordpunct_tokenize(tweet_text) if w.lower() in words)\n",
    "                        tweet_list.append(tweet_text.split())\n",
    "                        good_tweets += 1\n",
    "                except:\n",
    "                    bad_tweets += 1\n",
    "\n",
    "    print(good_tweets, bad_tweets)\n",
    "    print(\"after\", len(tweet_list))\n",
    "    return tweet_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training word embeddings using gensim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_model(tweetlist, title):\n",
    "    model = Word2Vec(tweetlist,\n",
    "                size=300,\n",
    "                window=10,\n",
    "                min_count=5,\n",
    "                workers=multiprocessing.cpu_count())\n",
    "    model.wv.save_word2vec_format(str(title)+'.txt', binary=False)\n",
    "    #save_word2vec_format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions to pickle and unpickle the lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickle_list(tweets, title):\n",
    "    with open(title+'.pkl', 'wb') as f:\n",
    "        pickle.dump(tweets, f)\n",
    "\n",
    "\n",
    "def unpickle_list(pickled_list):\n",
    "    with open(pickled_list, 'rb') as f:\n",
    "        unpickled_list = pickle.load(f)\n",
    "    return unpickled_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_glove_embedding(glovefile):\n",
    "    embeddings_index = dict()\n",
    "    f = api.load(glovefile)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between word similarities in Depressed vs General Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_similarity(word, model1, model2, model3):\n",
    "    result1 = model1.wv.similar_by_word(word, 20)\n",
    "    result2 = model2.wv.similar_by_word(word, 20)\n",
    "    result3 = model3.wv.similar_by_word(word, 20)\n",
    "\n",
    "    width = 20\n",
    "    print(\"word: \",word,'\\n')\n",
    "    print(\"{} {} {}\".format('healthy'.ljust(40), 'glove'.ljust(40), 'depressed'.ljust(width)))\n",
    "    print(\"-----------------------------------------------------------------------------------------------------------\")\n",
    "    for i, r in enumerate(result1):\n",
    "        print(\"{} {} | {} {} | {} {}\".format(\n",
    "            result1[i][0].ljust(width - 5),\n",
    "            str(result1[i][1]).ljust(width),\n",
    "            result2[i][0].ljust(width),\n",
    "            str(result2[i][1]).ljust(width-8),\n",
    "            result3[i][0].ljust(width),\n",
    "            str(result3[i][1]).ljust(width - 5)))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for training incremental word embeddings for contextual shifts over the years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makefolders(keyword, directory):\n",
    "    try:\n",
    "        os.mkdir(directory+keyword)\n",
    "    except OSError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_files(directory):\n",
    "    fileDir = os.path.dirname(os.path.realpath('__file__'))\n",
    "    print(fileDir)\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        if not file.startswith('.') and file[-3:] == '.gz':\n",
    "            keyword = file.split('-')[0]\n",
    "    \n",
    "            #print(os.path.join(directory, file))\n",
    "            if not os.path.exists(keyword):\n",
    "                #pass\n",
    "                makefolders(keyword, directory)\n",
    "                shutil.move(os.path.join(directory, file), directory+keyword)\n",
    "            else:\n",
    "                shutil.move(os.path.join(directory, file), directory+keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#divide_files(\"/users/s/a/sandhya/twitter_scrap/keyword-search/raw-tweets/healthy_group_sampled/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yearly_lists(d1, d2, years):\n",
    "    for year in years:\n",
    "        depressed_tweets = []\n",
    "        depressed_tweets = read_tweet_files(d1+year, depressed_tweets)\n",
    "        depressed_tweets = read_tweet_files(d2+year, depressed_tweets)\n",
    "        pickle_list(depressed_tweets, 'depress_tweets'+'_'+str(year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = ['2012','2013','2014','2015','2016','2017','2018','2019']\n",
    "#yearly_lists(depression_dir, depressed_dir, years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_incremental_model(tweets, title, pretrained_vectors):\n",
    "    model = Word2Vec(tweets,\n",
    "                size=300,\n",
    "                window=10,\n",
    "                min_count=5,\n",
    "                workers=multiprocessing.cpu_count())\n",
    "    \n",
    "    total_examples = model.corpus_count\n",
    "\n",
    "    pretrained = KeyedVectors.load_word2vec_format(pretrained_vectors, binary=False, unicode_errors='ignore')\n",
    "    model.build_vocab([list(pretrained.wv.vocab.keys())], update=True)\n",
    "    model.intersect_word2vec_format(pretrained_vectors, binary=False, lockf=1.0, unicode_errors='ignore')\n",
    "    \n",
    "    model.train(tweets, total_examples=total_examples, epochs=model.iter)\n",
    "    model.wv.save_word2vec_format(str(title)+'.txt', binary=False)\n",
    "    #model.save(str(title)+'.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_yearly_model(year):\n",
    "    if year == '2012':\n",
    "        tweets = unpickle_list('new_yearly_embeddings/cum_depress_tweets_'+year+'.pkl')\n",
    "        print(\"count of tweets in year\"+year,len(tweets))\n",
    "        build_model(tweets, 'cum_depress_tweets_'+year)\n",
    "    else:\n",
    "        tweets = unpickle_list('new_yearly_embeddings/cum_depress_tweets_'+year+'.pkl') \n",
    "        print(\"count of tweets in year\"+year,len(tweets))\n",
    "        build_incremental_model(tweets, 'incremental_depress_tweets_'+year, 'yearly_embedding_files/depress_tweets_'+year+'.bin')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_unicode(tweets):\n",
    "    new_tweets = []\n",
    "    for tweet in tweets:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            word = regex.sub('', word).strip()\n",
    "            if len(word) > 0:\n",
    "                temp.append(word)\n",
    "        new_tweets.append(temp)\n",
    "    return new_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_tweets = []\n",
    "for tweet in tweets:\n",
    "    temp = []\n",
    "    for word in tweet:\n",
    "        word = regex.sub('', word).strip()\n",
    "        if len(word) > 0:\n",
    "            temp.append(word)\n",
    "    new_tweets.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets = remove_unicode(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "years = ['2012','2013','2014','2015','2016','2017','2018','2019']\n",
    "for year in years:\n",
    "    build_yearly_model(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
